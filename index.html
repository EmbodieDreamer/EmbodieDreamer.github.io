<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling">
  <meta name="keywords" content="EmbodieDreamer">
  <!-- <meta name="google-site-verification" content="ciQsol-i_rwd2kkpVI_G-EFX4g72KA2HF-cDUi52lIo" /> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <title>EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling</h1>
            <!-- <h1 class="title is-2 publication-title"><strong><span style="color:#5e7dbd";>MotionStreamer</span></strong>: <span style="color:#EE822F";>Streaming Motion Generation </span> <br> via <span style="color:#069b1c";>Diffusion-based Autoregressive Model</span> <br> in <span style="color:#a50c07";>Causal Latent Space</span></h1> -->
            <!-- <span style="color: rgb(219, 13, 78); font-size: 1.2em;">Arxiv2025</span> -->
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Wp7sUlIAAAAJ&hl=en" target="_blank">Boyuan Wang<sup>1,2*</sup></a>,&nbsp</span>
              <span class="author-block">
                <a href="" target="_blank">Xinpan Meng<sup>1,2*</sup></a>,&nbsp</span>
              <span class="author-block">
                <a href="https://jeffwang987.github.io/" target="_blank">Xiaofeng Wang<sup>1,2*</sup></a>,&nbsp</span>
              <span class="author-block">
                <a href="http://www.zhengzhu.net/" target="_blank">Zheng Zhu<sup>1*</sup></a>,&nbsp</span>
              <span class="author-block">
                <a href="" target="_blank">Angen Ye<sup>1,2</sup></a>,&nbsp</span>
              <span class="author-block">
                <a href="" target="_blank">Yang Wang<sup>1</sup></a>,&nbsp</span>
              <br>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=DSjGPu0AAAAJ" target="_blank">Zhiqin Yang<sup>1</sup></a>,&nbsp</span>
              <span class="author-block">
                <a href="" target="_blank">Chaojun Ni<sup>1,3</sup></a>,&nbsp</span>
              <span class="author-block">
                <a href="" target="_blank">Guan Huang<sup>1</sup></a>,&nbsp</span>
                
                <span class="author-block">
                <a href="" target="_blank">Xingang Wang<sup>2</sup></a></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block" style="margin-right: 20px;"><sup>1</sup> GigaAI </span>
                    <span class="author-block" style="margin-right: 20px;"><sup>2</sup> Institute of Automation, Chinese Academy of Sciences, China </span>
                    <!-- <span class="author-block" style="margin-right: 20px;"><sup>3</sup> Luoyang Institute for Robot and Intelligent Equipment, China </span> -->
                    <!-- <span class="author-block" style="margin-right: 20px;"><sup>4</sup> GigaAI, China </span> -->
                    <span class="author-block" style="margin-right: 20px;"><sup>3</sup> Peking University, China </span>
                    <!-- <span class="author-block" style="margin-right: 20px;"><sup>6</sup> The Chinese University of Hong Kong </span> -->

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links" style="display: flex; justify-content: center; align-items: center; gap: 10px;">
                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2507.05198" class="external-link button is-normal is-rounded is-dark" target="_blank">               
                          <span class="icon">
                            <i class="ai ai-arxiv"></i>
                          </span>
                          <span>arXiv</span>
                        </a>
                      </span>
                      
                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2507.05198" class="external-link button is-normal is-rounded is-dark" target="_blank">
                          <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span>
                        </a>
                      </span>

                      <!-- Github link -->
                      <!-- <span class="link-block" style="display: flex; align-items: center; gap: 10px;"> -->
                        <!-- <a href="https://github.com/GigaAI-research/HumanDreamer" class="external-link button is-normal is-rounded is-dark" target="_blank">
                          <span class="icon">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>GitHub</span>
                        </a> -->
                        <!-- <a href="https://github.com/GigaAI-research/HumanDreamer" target="_blank" class="github-badge" style="background: #24292e; color: white; padding: 5px 10px; border-radius: 15px; text-decoration: none; font-size: 0.9em; display: flex; align-items: center; gap: 5px;">
                          <i class="fas fa-star" style="color: #f1e05a;"></i>
                          <span class="github-stars"></span>
                        </a> -->
                      <!-- </span> -->

                      <span class="link-block">
                        <a href="https://github.com/GigaAI-research/EmbodieDreamer" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>Code (Coming Soon)</span>
                        </a>
                      </span>
                      <!-- <span class="link-block">
                        <a href="https://github.com/GigaAI-research/HumanDreamer" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>Datset (Coming Soon)</span>
                        </a>
                      </span> -->
                    </div>
                  </div>
          </div>
        </div>
      </div>
    </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <img src="static/images/main_demo.png" width="90%" style="max-height: 800px;" alt="teaser">
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero ">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper method -->
<section class="section hero">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Method Overview</h2>
    
    <!-- PhysAligner 部分 -->
    <div style="margin-bottom: 40px;">
      <h3 class="title is-4 has-text-centered">PhysAligner</h3>
      <div style="display: flex; flex-direction: column; align-items: center; width: 90%; margin: 0 auto;">
        <img src="static/images/PhysAligner.png" style="width: 100%; max-height: 350px; object-fit: contain; margin-bottom: 2px;">
        <p style="margin-bottom: 20px; width: 100%; font-size: 0.9em; text-align: justify;">
          The figure illustrates the workflow of PhysAligner. First, a large amount of data is generated using a simulator. Then, a surrogate model is trained to fit the data. Finally, the physical parameters are optimized through gradient descent.
        </p>
        <video autoplay controls muted loop style="width: 100%; height: auto;">
          <source src="static/videos/compare_physaligner.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    
    <!-- VisAligner 部分 -->
    <div style="margin-bottom: 40px;">
      <h3 class="title is-4 has-text-centered">VisAligner</h3>
      <div style="display: flex; flex-direction: column; align-items: center; width: 90%; margin: 0 auto;">
        <img src="static/images/VisAligner.png" style="width: 100%; max-height: 350px; object-fit: contain; margin-bottom: 2px;">
        <p style="margin-bottom: 20px; width: 100%; font-size: 0.9em; text-align: justify;">
          The framework of VisAligner. A reference image containing the initial background and robot appearance information serves as the first frame of the conditioned video. The subsequent frames are generated by performing pixel-wise addition of the robot's motion observations from the simulated environment and the segmentation masks of the foreground objects. These frames are then encoded into latents via a VAE encoder, concatenated with noise along the channel dimension, and input to VisAligner for denoising. Spatial-temporal attention mechanisms are employed to capture long-range dependencies across both spatial and temporal dimensions, thereby enhancing the coherence and visual quality of the generated video. The final video is obtained by decoding the denoised latents.
        </p>
        <video autoplay controls muted loop style="width: 100%; height: auto;">
          <source src="static/videos/compare_seg.mp4" type="video/mp4">
        </video>
      </div>
    </div>

  </div>
  
  <div class="container">
    <h2 class="title is-3 has-text-centered">More Generation Results</h2>
    <div style="width: 90%; margin: 0 auto;">
      <!-- 添加视频说明 -->
      <p style="text-align: center; margin-bottom: 20px; font-size: 1em;">
        In each video: <strong>Left</strong> - GT video (Real world), <strong>Middle</strong> - Simulation video, <strong>Right</strong> - Generation video (Our EmbodieDreamer)
      </p>
      
      <!-- RT-1 机器人生成结果 -->
      <h3 class="title is-4 has-text-centered" style="margin-bottom: 10px;">Results on RT-1</h3>
      <!-- 第一行视频 -->
      <div class="columns">
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/rt1_0.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/rt1_1.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      
      <!-- 第二行视频 -->
      <div class="columns" style="margin-top: -60px;">
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/rt1_2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/rt1_3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      
      <!-- 第三行视频 -->
      <div class="columns" style="margin-top: -60px;">
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/rt1_4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/rt1_5.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      
      <!-- ALOHA 机器人生成结果 -->
      <h3 class="title is-4 has-text-centered" style="margin-top: 40px;">Results on Agilex Robot </h3>
      <!-- 第一行视频 -->
      <div class="columns">
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/aloha_0.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/aloha_1.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      
      <!-- 第二行视频 -->
      <div class="columns" style="margin-top: -60px;">
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/aloha_2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/aloha_3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      
      <!-- 第三行视频 -->
      <div class="columns" style="margin-top: -60px;">
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/aloha_4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video autoplay controls muted loop style="width: 100%; height: 150px; object-fit: contain;">
            <source src="static/videos/aloha_5.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
  
  <!-- 添加EmbodieDreamer with RL章节 -->
  <div class="container" style="margin-top: 40px;">
    <h2 class="title is-3 has-text-centered">EmbodieDreamer with RL</h2>
    <div style="width: 90%; margin: 0 auto;">
      <div style="display: flex; flex-direction: column; align-items: center;">
        <video autoplay controls muted style="width: 100%; max-height: 500px; object-fit: contain;">
          <source src="static/videos/RL_process.mov" type="video/mp4">
        </video>
        <p style="margin-top: 10px; width: 100%; font-size: 0.9em; text-align: justify;">
          Reinforcement learning process with EmbodieDreamer. Policy receives one-view visual input to produce
          actions, which are executed in the simulator to generate raw observations. These observations are
          then processed by VisAligner to render photorealistic videos. The last frame of the rendered video
          serves as the reference image for the next iteration of policy execution. This closed-loop process
          iterates for a fixed number of steps, yielding trajectories of consistent length with photorealistic visual
          observations.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{wang2025embodiedreamer,
        title={EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling}, 
        author={Boyuan Wang and Xinpan Meng and Xiaofeng Wang and Zheng Zhu and Angen Ye and Yang Wang and Zhiqin Yang and Chaojun Ni and Guan Huang and Xingang Wang},
        journal={arXiv preprint arXiv:2507.05198},
        year={2025},
        eprint={2507.05198}
      }
  </code></pre>
  </div>
</section>
<!--End BibTex citation -->


<style>
@keyframes flowingArrow {
  0% {
    opacity: 0.4;
    transform: translateX(-3px);
  }
  50% {
    opacity: 1;
    transform: translateX(3px);
  }
  100% {
    opacity: 0.4;
    transform: translateX(-3px);
  }
}

.videobox {
  width: 100%;
  height: 100%;
  padding: 0;
  margin: 0;
  overflow: hidden;
  background-color: transparent;
}

.videobox video {
  width: 100%;
  height: 100%;
  object-fit: cover;
  display: block;
  background-color: transparent;
}

.fixed-height {
  height: 300px;
  border: none;
  overflow: hidden;
  background-color: transparent;
}

.fixed-height video {
  width: 100%;
  height: 100%;
  object-fit: contain;
  border: none;
  background-color: transparent;
}

video {
  border: none;
  outline: none;
  background-color: transparent;
}
</style>

<script>
// 添加到页面的script部分
fetch('https://api.github.com/repos/Li-xingXiao/272-dim-Motion-Representation')
  .then(response => response.json())
  .then(data => {
    document.querySelector('.github-stars').textContent = data.stargazers_count;
  });
</script>

</body>
</html>
